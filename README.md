# Wheelchair-Arm-Control

A robot arm control library with functions of instruction interpretation, object recognition, arm motion planning and execution.
## Environment Setup
[Ubuntu 18.04.1 LTS](http://releases.ubuntu.com/18.04)  
[ROS Melodic](http://wiki.ros.org/melodic/Installation/Ubuntu)  
[ros_control](http://wiki.ros.org/ros_control)  
[moveit!](https://moveit.ros.org/install/)  
[gazebo_ros_pkgs](http://wiki.ros.org/gazebo_ros_pkgs)   
[Pointnet](https://github.com/charlesq34/pointnet)  
[pyopenssl](https://github.com/pyca/pyopenssl)  
[MQTT](https://github.com/mqtt/mqtt.github.io/wiki/software?id=software)  



## How to run  
0. clone to your ros catkin workspace and build the package  
```
cd ../catkin_ws/src
git clone https://github.com/scottlx/Wheelchair-Arm-Control.git  
cd ..  
catkin_make
```

1. source setup.bash
```
source ../catkin_ws/devel/setup.bash
```

## overview
### my_arm  
urdf files, mesh files, rviz model visualize launch file, gazebo launch file.  

Visualize the arm model in rviz:  

`roslaunch my_arm display.launch`  

Spawn the model into gazebo:  

`roslaunch my_arm gazebo.launch`  
### my_arm_control

low_level control  

`roslaunch my_arm_control my_arm_control.launch`  
It loads seven position controllers with controller_manager. We can publish messages to the controller to control the joints.  
After launching `gazebo.launch` and `my_arm_control.launch`  

`rosrun rqt_gui rqt_gui`  

Then apply a sinusoid wave to each joint  

![alt](/demo_img/publisher.png)  

Here is what you see in gazebo. Each joints is revoluting with a force of sinusoid wave:  
![alt](/demo_img/gazebo.gif)  

### arm_moveit_config  

Config file generated by moveit setup assistant  

play with the motion_planning plugin in rviz  

`roslaunch arm_moveit_config demo`  

![alt](/demo_img/moveit.gif)  

### Object Recognition using Pointnet

1. place a kinect in the gazebo Environment
2. get raw point cloud data and preprocess
   (seperate the data into small batches and do normalization etc.)
    Here is the original point cloud data
   ![alt](/demo_img/scene_grey.png)  
3. feed the preprocess point cloud data into the pointnet
    Here is the point cloud labeled by different colors:
   ![alt](/demo_img/scene_color.png)  

4. get the location of interested object according to labels' of points



## Goals to achieve  

1. Apply in a real robotic arm.

## Alexa

1.Go to Alexa developer console to create a new skill https://developer.amazon.com/alexa/console/ask
2.Go to A mazon Web Service to create new Lambda function and ioT service https://console.aws.amazon.com/console/home?region=us-east-1#
3.Use Alexa_skill.json to deploy your new skill
4.Upload Lambda_arm_control.zip to deploy your Lambda function
5.Connect three part together, and now you can see the topic published in AWS ioT MQTT client when you give new voice command to the Alexa
