# Wheelchair-Arm-Control

A robot arm control library with functions of instruction interpretation, object recognition, arm motion planning and execution.  

## Video demonstration  
[https://youtu.be/ml04Gt5j70s](https://youtu.be/ml04Gt5j70s)
## Environment Setup
[Ubuntu 18.04.1 LTS](http://releases.ubuntu.com/18.04)  
[python 2.7](https://www.python.org/download/releases/2.7/)  
[ROS Melodic](http://wiki.ros.org/melodic/Installation/Ubuntu)  
[ros_control](http://wiki.ros.org/ros_control)  
[moveit!](https://moveit.ros.org/install/)  
[gazebo_ros_pkgs](http://wiki.ros.org/gazebo_ros_pkgs)   
[Pointnet](https://github.com/charlesq34/pointnet)  
[pyopenssl](https://github.com/pyca/pyopenssl)  
[MQTT](https://github.com/mqtt/mqtt.github.io/wiki/software?id=software)  
[tensorflow 1.0](https://www.tensorflow.org/api_guides/python/upgrade)  
[meshlab](http://www.meshlab.net/)  



## How to run  
0. clone to your ros catkin workspace and build the package  
```
cd ../catkin_ws/src
git clone https://github.com/scottlx/Wheelchair-Arm-Control.git  
cd ..  
catkin_make
```

1. source setup.bash
```
source ../catkin_ws/devel/setup.bash
```

## overview
### my_arm  
urdf files, mesh files, rviz model visualize launch file, gazebo launch file.  

Visualize the arm model in rviz:  

`roslaunch my_arm display.launch`  

Spawn the model into gazebo:  

`roslaunch my_arm gazebo.launch`  

### arm_moveit_config  

Config file generated by moveit setup assistant  

play with the motion_planning plugin in rviz  

`roslaunch arm_moveit_config execution.launch`  

![alt](/demo_img/moveit.gif)  

### python_interface  
A python execution script which integrates the whole system  

`python MQTT_sub.py`

### Object Recognition using Pointnet

1. place a kinect in the gazebo Environment
2. get raw point cloud data and preprocess
   (seperate the data into small batches and do normalization etc.)
    Here is the original point cloud data
   ![alt](/demo_img/scene_grey.png)  
3. feed the preprocess point cloud data into the pointnet
    Here is the point cloud labeled by different colors:
   ![alt](/demo_img/scene_color.png)  

4. get the location of interested object according to labels' of points



## Goals to achieve  

1. Apply in a real robotic arm.

## Alexa

1.Go to Alexa developer console to create a new skill https://developer.amazon.com/alexa/console/ask
2.Go to A mazon Web Service to create new Lambda function and ioT service https://console.aws.amazon.com/console/home?region=us-east-1#
3.Use Alexa_skill.json to deploy your new skill
4.Upload Lambda_arm_control.zip to deploy your Lambda function
5.Connect three part together, and now you can see the topic published in AWS ioT MQTT client when you give new voice command to the Alexa
